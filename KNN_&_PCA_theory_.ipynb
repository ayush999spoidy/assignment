{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is K-Nearest Neighbors (KNN) and how does it work."
      ],
      "metadata": {
        "id": "MYHV37IJ4bHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.K-Nearest Neighbors (KNN) is a machine learning algorithm used for classification and regression. It's non-parametric (makes no assumptions about data) and lazy (doesn't build a model beforehand).\n",
        "\n",
        "Distance Calculation: KNN calculates the distance between a new data point and all points in the existing dataset.\n",
        "\n",
        "\n",
        "Neighbor Identification: It finds the 'k' nearest data points to the new point (k is a user-defined number)."
      ],
      "metadata": {
        "id": "W8Y1eTVC4a-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the difference between KNN Classification and KNN Regression."
      ],
      "metadata": {
        "id": "x8S5wQ4E6Efg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.KNN Classification:\n",
        "\n",
        "Objective: To predict the class label of a new data point.\n",
        "How it works:\n",
        "Finds the 'k' nearest neighbors to the new data point.\n",
        "Assigns the new data point to the most frequent class among its neighbors (a majority vote).\n",
        "Example: Classifying an email as spam or not spam based on the content and sender information.\n",
        "\n",
        "\n",
        "KNN Regression:\n",
        "\n",
        "Objective: To predict a continuous value for a new data point.\n",
        "How it works:\n",
        "Finds the 'k' nearest neighbors to the new data point.\n",
        "Predicts the value of the new data point by averaging the values of its neighbors."
      ],
      "metadata": {
        "id": "E3Yl0rql6FC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What is the role of the distance metric in KNN."
      ],
      "metadata": {
        "id": "qxjRZHRX6klq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.The distance metric is a crucial component of the KNN algorithm. It defines how the algorithm measures the similarity or dissimilarity between data points. This measurement is fundamental to identifying the 'k' nearest neighbors."
      ],
      "metadata": {
        "id": "fkJWpiZ36lEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What is the Curse of Dimensionality in KNN."
      ],
      "metadata": {
        "id": "n17sFN9H6-59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.The Curse of Dimensionality refers to the phenomenon where the performance of KNN (and other distance-based algorithms) deteriorates as the number of features (dimensions) in the dataset increases."
      ],
      "metadata": {
        "id": "CDsB8mj36_Xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.How can we choose the best value of K in KNN."
      ],
      "metadata": {
        "id": "dxSvc7SU7O8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.The value of 'k' in KNN, representing the number of nearest neighbors to consider, is a crucial hyperparameter that significantly influences the model's performance. Choosing the optimal value of 'k' is essential for achieving good accuracy and generalization."
      ],
      "metadata": {
        "id": "k915naCG7PSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What are KD Tree and Ball Tree in KNN."
      ],
      "metadata": {
        "id": "m2qcDJLp7mqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.KD Tree and Ball Tree are efficient data structures that can significantly speed up the nearest neighbor search in KNN, especially for large and high-dimensional datasets. Choosing the appropriate data structure depends on the characteristics of your data and the problem you are solving."
      ],
      "metadata": {
        "id": "rL7-oU8E7nK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.When should you use KD Tree vs. Ball Tree"
      ],
      "metadata": {
        "id": "ohaO3GGY8hYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.KD Tree:\n",
        "\n",
        "Preferred for low to moderate dimensionality (typically less than 20 dimensions).\n",
        "In lower dimensions, KD Trees are generally faster and more efficient for nearest neighbor search.\n",
        "However, as the dimensionality increases, the performance of KD Trees can degrade significantly due to the curse of dimensionality.\n",
        "\n",
        "\n",
        "Ball Tree:\n",
        "\n",
        "Preferred for high dimensionality (more than 20 dimensions).\n",
        "Ball Trees are more robust to the curse of dimensionality and tend to perform better in higher-dimensional spaces.\n",
        "They are designed to handle datasets with a large number of features more efficiently."
      ],
      "metadata": {
        "id": "T5qkiX6S8hnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What are the disadvantages of KNN."
      ],
      "metadata": {
        "id": "zhKohuzr8x5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.KNN remains a popular algorithm due to its simplicity, versatility, and ease of implementation. By understanding its limitations and applying appropriate techniques to mitigate them, you can effectively use KNN for various classification and regression tasks."
      ],
      "metadata": {
        "id": "OaP3QKEJ8yds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.How does feature scaling affect KNN."
      ],
      "metadata": {
        "id": "MzdBDg_G9E3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And.Feature scaling is a crucial preprocessing step for KNN because the algorithm is distance-based. If the data is not scaled, features with larger scales will dominate the distance calculation and can result in incorrect classifications."
      ],
      "metadata": {
        "id": "_SrQYUvx9Frm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What is PCA (Principal Component Analysis)."
      ],
      "metadata": {
        "id": "Ekd5467y9fDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while retaining as much of the important information as possible. It's a powerful tool for data visualization, noise reduction, and feature extraction."
      ],
      "metadata": {
        "id": "GYZrenyc9fLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.How does PCA work."
      ],
      "metadata": {
        "id": "yjXyH9nY9xuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.PCA aims to find the directions (principal components) in the data that capture the maximum variance. These principal components are essentially new features that are linear combinations of the original features. By projecting the data onto these principal components, we can reduce the dimensionality of the data while retaining as much of the important information as possible."
      ],
      "metadata": {
        "id": "QkgIhrQw9yU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.What is the geometric intuition behind PCA."
      ],
      "metadata": {
        "id": "rKJ9ilkX-AMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Imagine your data points as a cloud of points in a high-dimensional space. PCA aims to find the best-fitting lower-dimensional subspace (a line, a plane, or a hyperplane) that captures the most variance in the data."
      ],
      "metadata": {
        "id": "qrI48lBQ-A5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What is the difference between Feature Selection and Feature Extraction."
      ],
      "metadata": {
        "id": "wfTqYNwO_ua-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Feature Selection is preferred when you want to retain the original features and their interpretability. It's also useful when you have domain knowledge that can guide the selection of relevant features.\n",
        "\n",
        "\n",
        "Feature Extraction is preferred when you want to create new features that are more informative and less correlated than the original features. It's also useful when you have a large number of features and want to reduce the dimensionality of the data significantly."
      ],
      "metadata": {
        "id": "fJ-YQRY0_uzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.What are Eigenvalues and Eigenvectors in PCA."
      ],
      "metadata": {
        "id": "xaK4ZxUO_-Yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Eigenvalues and eigenvectors are fundamental concepts in linear algebra that play a crucial role in PCA. They are used to identify the principal components, which are the directions of greatest variance in the data."
      ],
      "metadata": {
        "id": "b8IRNXwJ_-3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How do you decide the number of components to keep in PCA."
      ],
      "metadata": {
        "id": "F0VLJhpsAMgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.By carefully considering these approaches and factors, you can make an informed decision about the number of components to retain in PCA, balancing dimensionality reduction with the preservation of important information for your specific task."
      ],
      "metadata": {
        "id": "oHZd9LZKAM1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Can PCA be used for classification."
      ],
      "metadata": {
        "id": "HxcG7o7IAu_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.PCA can be a valuable tool for classification as a preprocessing step to reduce dimensionality, extract features, and visualize the data. While it might not always improve accuracy, it can often lead to faster training and better performance of classifiers, especially in high-dimensional datasets."
      ],
      "metadata": {
        "id": "642ExHpPAvUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.What are the limitations of PCA."
      ],
      "metadata": {
        "id": "07lXTpx6A_y7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.PCA remains a valuable tool for dimensionality reduction and feature extraction. By understanding its limitations and applying appropriate techniques to mitigate them, you can effectively use PCA for various data analysis and machine learning tasks."
      ],
      "metadata": {
        "id": "cbJFk88eBAQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.How do KNN and PCA complement each other."
      ],
      "metadata": {
        "id": "CidJ6vrdBRj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. PCA and KNN can work together to overcome the limitations of KNN in high-dimensional datasets. PCA reduces dimensionality and extracts informative features, while KNN uses these features for classification or regression. This combination can lead to improved accuracy, efficiency, and robustness of the KNN model."
      ],
      "metadata": {
        "id": "190p8m6oBR8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How does KNN handle missing values in a dataset."
      ],
      "metadata": {
        "id": "j15StHTABvSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.KNN is sensitive to missing values because it relies on distance calculations between data points. If a data point has missing values for some features, it can be difficult to calculate the distance to other data points."
      ],
      "metadata": {
        "id": "U936_GoFBvrl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What are the key differences between PCA and Linear Discriminant Analysis (LDA)."
      ],
      "metadata": {
        "id": "wfi6uJr6B8Yx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YAcgiFW38t2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.PCA focuses on finding the directions of greatest variance in the data, regardless of class labels. It's like finding the axes of an ellipsoid that best fit the data cloud.\n",
        "LDA focuses on finding the directions that best separate different classes. It's like finding the lines that best divide the data points into their respective classes."
      ],
      "metadata": {
        "id": "9mVi_OWgB8uQ"
      }
    }
  ]
}